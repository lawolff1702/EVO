{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ee01e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36ac1fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import numpy as np\n",
    "\n",
    "from EVO import LogisticRegression, EvolutionOptimizer, DeepNeuralNetwork\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88092835",
   "metadata": {},
   "source": [
    "## Preparing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "139e65d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n",
      "100.0%\n",
      "100.0%\n",
      "100.0%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_exp shape: torch.Size([60000, 7850])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define a transform that converts images to a tensor and flattens them.\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),                    # MNIST images are [1, 28, 28]\n",
    "    transforms.Lambda(lambda x: x.view(-1))     # Flatten to [784]\n",
    "])\n",
    "\n",
    "# Load the MNIST training and test sets.\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset  = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "def add_bias(X):\n",
    "    # X is of shape (N, 784), append a column of ones to yield shape (N, 785)\n",
    "    N = X.size(0)\n",
    "    bias = torch.ones(N, 1)\n",
    "    return torch.cat([X, bias], dim=1)\n",
    "\n",
    "# Build full training tensors.\n",
    "X_train = torch.stack([train_dataset[i][0] for i in range(len(train_dataset))])\n",
    "X_train = add_bias(X_train)  # Now shape: (N_train, 785)\n",
    "y_train = torch.tensor([float(train_dataset[i][1] > 4) for i in range(len(train_dataset))])\n",
    "\n",
    "# Build full test tensors.\n",
    "X_test = torch.stack([test_dataset[i][0] for i in range(len(test_dataset))])\n",
    "X_test = add_bias(X_test)   # Now shape: (N_test, 785)\n",
    "y_test = torch.tensor([float(test_dataset[i][1] > 4) for i in range(len(test_dataset))])\n",
    "\n",
    "def expand_features(X, num_classes=10):\n",
    "    \"\"\"\n",
    "    Expand features from shape (N, d) to (N, d * num_classes) by forming blocks.\n",
    "    Here we multiply each block by a unique constant so that each block is distinct.\n",
    "    \"\"\"\n",
    "    N, d = X.shape\n",
    "    blocks = []\n",
    "    for c in range(num_classes):\n",
    "        # Multiply the original features by (c+1)\n",
    "        blocks.append( (c+1) * X )\n",
    "    return torch.cat(blocks, dim=1)\n",
    "\n",
    "# Expand the feature matrices so that each sample now has 7850 features.\n",
    "X_train_exp = expand_features(X_train, num_classes=10)  # shape: (N_train, 7850)\n",
    "X_test_exp  = expand_features(X_test, num_classes=10)   # shape: (N_test, 7850)\n",
    "\n",
    "# (Optional) Visualize one original image.\n",
    "img = X_train[0][:-1].view(28, 28)  # exclude bias from visualization\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.title(f\"True Label: {y_train[0].item()}\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print(\"X_train_exp shape:\", X_train_exp.shape)  # Should be (N_train, 7850)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36291f54",
   "metadata": {},
   "source": [
    "## Linear Regression and Evolutionary Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a9c5270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch 01: Loss = 2.7270, Training Accuracy = 49.01%\n",
      "Epoch 02: Loss = 1.4804, Training Accuracy = 49.01%\n",
      "Epoch 03: Loss = 0.5448, Training Accuracy = 49.01%\n",
      "Epoch 04: Loss = -0.2553, Training Accuracy = 49.01%\n",
      "Epoch 05: Loss = -0.9520, Training Accuracy = 49.01%\n",
      "Epoch 06: Loss = -1.5106, Training Accuracy = 49.01%\n",
      "Epoch 07: Loss = -2.0796, Training Accuracy = 49.01%\n",
      "Epoch 08: Loss = -2.5290, Training Accuracy = 49.01%\n",
      "Epoch 09: Loss = -2.8959, Training Accuracy = 49.01%\n",
      "Epoch 10: Loss = -3.3930, Training Accuracy = 49.01%\n",
      "Epoch 11: Loss = -3.7192, Training Accuracy = 49.01%\n",
      "Epoch 12: Loss = -3.9452, Training Accuracy = 49.01%\n",
      "Epoch 13: Loss = -4.3062, Training Accuracy = 49.01%\n",
      "Epoch 14: Loss = -4.6369, Training Accuracy = 49.01%\n",
      "Epoch 15: Loss = -4.9634, Training Accuracy = 49.01%\n",
      "Epoch 16: Loss = -5.0183, Training Accuracy = 49.01%\n",
      "Epoch 17: Loss = -5.2689, Training Accuracy = 49.01%\n",
      "Epoch 18: Loss = -5.2576, Training Accuracy = 49.01%\n",
      "Epoch 19: Loss = -5.4169, Training Accuracy = 49.01%\n",
      "Epoch 20: Loss = -5.7884, Training Accuracy = 49.01%\n",
      "Epoch 21: Loss = -5.9238, Training Accuracy = 49.01%\n",
      "Epoch 22: Loss = -6.1323, Training Accuracy = 49.01%\n",
      "Epoch 23: Loss = -6.1829, Training Accuracy = 49.01%\n",
      "Epoch 24: Loss = -6.2667, Training Accuracy = 49.01%\n",
      "Epoch 25: Loss = -6.3909, Training Accuracy = 49.01%\n",
      "Epoch 26: Loss = -6.4120, Training Accuracy = 49.01%\n",
      "Epoch 27: Loss = -6.4543, Training Accuracy = 49.01%\n",
      "Epoch 28: Loss = -6.5178, Training Accuracy = 49.01%\n",
      "Epoch 29: Loss = -6.6248, Training Accuracy = 49.01%\n",
      "Epoch 30: Loss = -7.0388, Training Accuracy = 49.01%\n",
      "Epoch 31: Loss = -7.2217, Training Accuracy = 49.01%\n",
      "Epoch 32: Loss = -7.1541, Training Accuracy = 49.01%\n",
      "Epoch 33: Loss = -7.0750, Training Accuracy = 49.01%\n",
      "Epoch 34: Loss = -7.0850, Training Accuracy = 49.01%\n",
      "Epoch 35: Loss = -7.3639, Training Accuracy = 49.01%\n",
      "Epoch 36: Loss = -7.4919, Training Accuracy = 49.01%\n",
      "Epoch 37: Loss = -7.6662, Training Accuracy = 49.01%\n",
      "Epoch 38: Loss = -7.5084, Training Accuracy = 49.01%\n",
      "Epoch 39: Loss = -7.3409, Training Accuracy = 49.01%\n",
      "Epoch 40: Loss = -7.6668, Training Accuracy = 49.01%\n",
      "Epoch 41: Loss = -7.5103, Training Accuracy = 49.01%\n",
      "Epoch 42: Loss = -7.5313, Training Accuracy = 49.01%\n",
      "Epoch 43: Loss = -7.5288, Training Accuracy = 49.01%\n",
      "Epoch 44: Loss = -7.5760, Training Accuracy = 49.01%\n",
      "Epoch 45: Loss = -7.5674, Training Accuracy = 49.01%\n",
      "Epoch 46: Loss = -7.5725, Training Accuracy = 49.01%\n",
      "Epoch 47: Loss = -7.4343, Training Accuracy = 49.01%\n",
      "Epoch 48: Loss = -7.6721, Training Accuracy = 49.01%\n",
      "Epoch 49: Loss = -7.5959, Training Accuracy = 49.01%\n",
      "Epoch 50: Loss = -7.6210, Training Accuracy = 49.01%\n",
      "Epoch 51: Loss = -7.5834, Training Accuracy = 49.01%\n",
      "Epoch 52: Loss = -7.5995, Training Accuracy = 49.01%\n",
      "Epoch 53: Loss = -7.5944, Training Accuracy = 49.01%\n",
      "Epoch 54: Loss = -7.4767, Training Accuracy = 49.01%\n",
      "Epoch 55: Loss = -7.7766, Training Accuracy = 49.01%\n",
      "Epoch 56: Loss = -7.8402, Training Accuracy = 49.01%\n",
      "Epoch 57: Loss = -7.8536, Training Accuracy = 49.01%\n",
      "Epoch 58: Loss = -7.9172, Training Accuracy = 49.01%\n",
      "Epoch 59: Loss = -7.9166, Training Accuracy = 49.01%\n",
      "Epoch 60: Loss = -7.8121, Training Accuracy = 49.01%\n",
      "Epoch 61: Loss = -7.9812, Training Accuracy = 49.01%\n",
      "Epoch 62: Loss = -7.8464, Training Accuracy = 49.01%\n",
      "Epoch 63: Loss = -7.8925, Training Accuracy = 49.01%\n",
      "Epoch 64: Loss = -7.6500, Training Accuracy = 49.01%\n",
      "Epoch 65: Loss = -7.6453, Training Accuracy = 49.01%\n",
      "Epoch 66: Loss = -7.4889, Training Accuracy = 49.01%\n",
      "Epoch 67: Loss = -7.5916, Training Accuracy = 49.01%\n",
      "Epoch 68: Loss = -7.5411, Training Accuracy = 49.01%\n",
      "Epoch 69: Loss = -7.6788, Training Accuracy = 49.02%\n",
      "Epoch 70: Loss = -7.7340, Training Accuracy = 49.01%\n",
      "Epoch 71: Loss = -7.2552, Training Accuracy = 49.01%\n",
      "Epoch 72: Loss = -7.2185, Training Accuracy = 49.01%\n",
      "Epoch 73: Loss = -7.5621, Training Accuracy = 49.06%\n",
      "Epoch 74: Loss = -7.3178, Training Accuracy = 49.05%\n",
      "Epoch 75: Loss = -7.1352, Training Accuracy = 49.05%\n",
      "Epoch 76: Loss = -6.4736, Training Accuracy = 49.06%\n",
      "Epoch 77: Loss = -6.1270, Training Accuracy = 49.25%\n",
      "Epoch 78: Loss = -6.3525, Training Accuracy = 49.59%\n",
      "Epoch 79: Loss = -6.4776, Training Accuracy = 50.06%\n",
      "Epoch 80: Loss = -6.7699, Training Accuracy = 50.67%\n",
      "Epoch 81: Loss = -6.8805, Training Accuracy = 50.78%\n",
      "Epoch 82: Loss = -7.1956, Training Accuracy = 51.54%\n",
      "Epoch 83: Loss = -7.3547, Training Accuracy = 52.03%\n",
      "Epoch 84: Loss = -7.5244, Training Accuracy = 52.24%\n",
      "Epoch 85: Loss = -7.5883, Training Accuracy = 53.95%\n",
      "Epoch 86: Loss = -7.9174, Training Accuracy = 54.43%\n",
      "Epoch 87: Loss = -7.7153, Training Accuracy = 55.28%\n",
      "Epoch 88: Loss = -7.6117, Training Accuracy = 55.77%\n",
      "Epoch 89: Loss = -7.4591, Training Accuracy = 56.16%\n",
      "Epoch 90: Loss = -7.7739, Training Accuracy = 56.35%\n",
      "Epoch 91: Loss = -7.8993, Training Accuracy = 56.86%\n",
      "Epoch 92: Loss = -8.0224, Training Accuracy = 57.18%\n",
      "Epoch 93: Loss = -8.0923, Training Accuracy = 57.15%\n",
      "Epoch 94: Loss = -8.2513, Training Accuracy = 57.45%\n",
      "Epoch 95: Loss = -8.2206, Training Accuracy = 57.86%\n",
      "Epoch 96: Loss = -8.1470, Training Accuracy = 57.95%\n",
      "Epoch 97: Loss = -8.0056, Training Accuracy = 58.34%\n",
      "Epoch 98: Loss = -8.2867, Training Accuracy = 58.64%\n",
      "Epoch 99: Loss = -8.3508, Training Accuracy = 58.53%\n",
      "Epoch 100: Loss = -8.4168, Training Accuracy = 58.61%\n",
      "Epoch 101: Loss = -8.5999, Training Accuracy = 58.62%\n",
      "Epoch 102: Loss = -8.5680, Training Accuracy = 58.69%\n",
      "Epoch 103: Loss = -8.6887, Training Accuracy = 58.84%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     27\u001b[39m train_accs = []\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     optimizer.step(X_train_exp, y_train)\n\u001b[32m     32\u001b[39m     \u001b[38;5;66;03m# Compute the current loss and accuracy on the training set.\u001b[39;00m\n\u001b[32m     33\u001b[39m     loss_val = model.loss(X_train_exp, y_train).item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/EVO/ExplorationVsExploitation/EVO.py:244\u001b[39m, in \u001b[36mEvolutionOptimizer.step\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    240\u001b[39m     \u001b[38;5;28mself\u001b[39m.population = [torch.rand(X.size(\u001b[32m1\u001b[39m), device=\u001b[38;5;28mself\u001b[39m.device) \n\u001b[32m    241\u001b[39m                     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.population_size)]\n\u001b[32m    243\u001b[39m \u001b[38;5;66;03m# Build tuples containing (loss, unique_id, candidate) so that ties can be broken.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m pop_with_losses = [(\u001b[38;5;28mself\u001b[39m.model.loss(X, y, w).item(), i, w) \n\u001b[32m    245\u001b[39m                 \u001b[38;5;28;01mfor\u001b[39;00m i, w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m.population)]\n\u001b[32m    246\u001b[39m \u001b[38;5;66;03m# Use heapq to extract the best half.\u001b[39;00m\n\u001b[32m    247\u001b[39m best_half = [w \u001b[38;5;28;01mfor\u001b[39;00m (_, _, w) \u001b[38;5;129;01min\u001b[39;00m heapq.nsmallest(\u001b[38;5;28mself\u001b[39m.population_size // \u001b[32m2\u001b[39m, pop_with_losses)]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/EVO/ExplorationVsExploitation/EVO.py:244\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    240\u001b[39m     \u001b[38;5;28mself\u001b[39m.population = [torch.rand(X.size(\u001b[32m1\u001b[39m), device=\u001b[38;5;28mself\u001b[39m.device) \n\u001b[32m    241\u001b[39m                     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.population_size)]\n\u001b[32m    243\u001b[39m \u001b[38;5;66;03m# Build tuples containing (loss, unique_id, candidate) so that ties can be broken.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m pop_with_losses = [(\u001b[38;5;28mself\u001b[39m.model.loss(X, y, w).item(), i, w) \n\u001b[32m    245\u001b[39m                 \u001b[38;5;28;01mfor\u001b[39;00m i, w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m.population)]\n\u001b[32m    246\u001b[39m \u001b[38;5;66;03m# Use heapq to extract the best half.\u001b[39;00m\n\u001b[32m    247\u001b[39m best_half = [w \u001b[38;5;28;01mfor\u001b[39;00m (_, _, w) \u001b[38;5;129;01min\u001b[39;00m heapq.nsmallest(\u001b[38;5;28mself\u001b[39m.population_size // \u001b[32m2\u001b[39m, pop_with_losses)]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/EVO/ExplorationVsExploitation/EVO.py:95\u001b[39m, in \u001b[36mLogisticRegression.loss\u001b[39m\u001b[34m(self, X, y, w)\u001b[39m\n\u001b[32m     92\u001b[39m preds = torch.clamp(\u001b[38;5;28mself\u001b[39m.sigmoid(X @ w), \u001b[32m1e-7\u001b[39m, \u001b[32m1\u001b[39m - \u001b[32m1e-7\u001b[39m)  \u001b[38;5;66;03m# Avoid log(0)\u001b[39;00m\n\u001b[32m     94\u001b[39m \u001b[38;5;66;03m# Adding a term to penalize the model for low diversity in the population\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m diversity_term = \u001b[38;5;28mself\u001b[39m.optimizer.average_pairwise_distance()\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (-y * torch.log(preds) - (\u001b[32m1\u001b[39m - y) * torch.log(\u001b[32m1\u001b[39m - preds)).mean() - (\u001b[38;5;28mself\u001b[39m.diversity_coeff * diversity_term)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/EVO/ExplorationVsExploitation/EVO.py:213\u001b[39m, in \u001b[36mEvolutionOptimizer.average_pairwise_distance\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    211\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n < \u001b[32m2\u001b[39m:\n\u001b[32m    212\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m dists = [torch.norm(\u001b[38;5;28mself\u001b[39m.population[i] - \u001b[38;5;28mself\u001b[39m.population[j])\n\u001b[32m    214\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n) \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(i + \u001b[32m1\u001b[39m, n)]\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m torch.stack(dists).mean()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/EVO/ExplorationVsExploitation/EVO.py:213\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    211\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n < \u001b[32m2\u001b[39m:\n\u001b[32m    212\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m dists = [torch.norm(\u001b[38;5;28mself\u001b[39m.population[i] - \u001b[38;5;28mself\u001b[39m.population[j])\n\u001b[32m    214\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n) \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(i + \u001b[32m1\u001b[39m, n)]\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m torch.stack(dists).mean()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/EVO/lib/python3.11/site-packages/torch/functional.py:1827\u001b[39m, in \u001b[36mnorm\u001b[39m\u001b[34m(input, p, dim, keepdim, out, dtype)\u001b[39m\n\u001b[32m   1823\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m p == \u001b[33m\"\u001b[39m\u001b[33mfro\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[32m   1824\u001b[39m     dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dim, (\u001b[38;5;28mint\u001b[39m, torch.SymInt)) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(dim) <= \u001b[32m2\u001b[39m\n\u001b[32m   1825\u001b[39m ):\n\u001b[32m   1826\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1827\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m torch.linalg.vector_norm(\n\u001b[32m   1828\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[32m2\u001b[39m, _dim, keepdim, dtype=dtype\n\u001b[32m   1829\u001b[39m         )\n\u001b[32m   1830\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1831\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m torch.linalg.vector_norm(\n\u001b[32m   1832\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[32m2\u001b[39m, _dim, keepdim, dtype=dtype, out=out\n\u001b[32m   1833\u001b[39m         )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Choose device: use MPS if available (Apple Silicon), else CPU.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\"))\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Instantiate the model on the chosen device.\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Import your EvolutionOptimizer from EVO.py (unchanged).\n",
    "from EVO import EvolutionOptimizer\n",
    "\n",
    "# Instantiate the evolutionary optimizer.\n",
    "optimizer = EvolutionOptimizer(model)\n",
    "optimizer.set_population_size(30)\n",
    "optimizer.set_mutation_rate(0.3)\n",
    "optimizer.set_mutation_intensity(0.6)\n",
    "optimizer.set_diversity_coeff(0.1)\n",
    "model.set_optimizer(optimizer)\n",
    "\n",
    "# Ensure your expanded training data is on the same device.\n",
    "X_train_exp = X_train_exp.to(device)   # X_train_exp should have shape (N_train, 7850)\n",
    "y_train = y_train.to(device)\n",
    "\n",
    "num_epochs = 2000\n",
    "losses = []\n",
    "train_accs = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.step(X_train_exp, y_train)\n",
    "    \n",
    "    # Compute the current loss and accuracy on the training set.\n",
    "    loss_val = model.loss(X_train_exp, y_train).item()\n",
    "    losses.append(loss_val)\n",
    "    \n",
    "    preds = model.predict(X_train_exp)\n",
    "    acc = (preds == y_train).float().mean().item() * 100\n",
    "    train_accs.append(acc)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1:02d}: Loss = {loss_val:.4f}, Training Accuracy = {acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb69253",
   "metadata": {},
   "source": [
    "## Deep Neural Network with Evolutionary Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f897c7f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "EvolutionOptimizer.__init__() got an unexpected keyword argument 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m model = DeepNeuralNetwork(layer_dims=[input_dim, \u001b[32m128\u001b[39m, \u001b[32m64\u001b[39m, \u001b[32m1\u001b[39m])\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Attach optimizer\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m optimizer = EvolutionOptimizer(model, device=device)\n\u001b[32m      8\u001b[39m optimizer.set_population_size(\u001b[32m30\u001b[39m)\n\u001b[32m      9\u001b[39m optimizer.set_mutation_rate(\u001b[32m0.6\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: EvolutionOptimizer.__init__() got an unexpected keyword argument 'device'"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "input_dim = X_train_exp.shape[1]\n",
    "model = DeepNeuralNetwork(layer_dims=[input_dim, 128, 64, 1])\n",
    "\n",
    "# Attach optimizer\n",
    "optimizer = EvolutionOptimizer(model, device=device)\n",
    "optimizer.set_population_size(30)\n",
    "optimizer.set_mutation_rate(0.6)\n",
    "optimizer.set_mutation_intensity(0.8)\n",
    "optimizer.set_diversity_coeff(0.1)\n",
    "model.set_optimizer(optimizer)\n",
    "\n",
    "# Fix: Pre-initialize population with correct shape\n",
    "param_size = model.w.numel()\n",
    "optimizer.population = [\n",
    "    torch.rand(param_size, device=device) for _ in range(optimizer.population_size)\n",
    "]\n",
    "model.set_optimizer(optimizer)\n",
    "\n",
    "# Move data to device\n",
    "X_train_exp = X_train_exp.to(device)\n",
    "y_train = y_train.to(device)\n",
    "\n",
    "# Train loop\n",
    "num_epochs = 1000\n",
    "losses = []\n",
    "train_accs = []\n",
    "BCE_loss = []\n",
    "diversity_loss = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.step(X_train_exp, y_train)\n",
    "\n",
    "    loss_val = model.loss(X_train_exp, y_train).item()\n",
    "    losses.append(loss_val)\n",
    "\n",
    "    preds = model.predict(X_train_exp)\n",
    "    acc = (preds == y_train).float().mean().item() * 100\n",
    "    train_accs.append(acc)\n",
    "    optimizer.set_diversity_coeff(max(0.01 * (1 - epoch / 300),0))\n",
    "    diversity_loss.append(model.curr_diversity)\n",
    "    BCE_loss.append(model.curr_bce)\n",
    "\n",
    "    print(f\"Epoch {epoch+1:03d}: Loss = {loss_val:.4f}, Accuracy = {acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "193e95f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: Loss = 0.6928, Accuracy = 51.77%\n",
      "Epoch 002: Loss = 0.6925, Accuracy = 52.24%\n",
      "Epoch 003: Loss = 0.6920, Accuracy = 52.23%\n",
      "Epoch 004: Loss = 0.6913, Accuracy = 52.16%\n",
      "Epoch 005: Loss = 0.6905, Accuracy = 52.20%\n",
      "Epoch 006: Loss = 0.6894, Accuracy = 52.54%\n",
      "Epoch 007: Loss = 0.6880, Accuracy = 53.35%\n",
      "Epoch 008: Loss = 0.6860, Accuracy = 54.97%\n",
      "Epoch 009: Loss = 0.6833, Accuracy = 57.86%\n",
      "Epoch 010: Loss = 0.6793, Accuracy = 61.87%\n",
      "Epoch 011: Loss = 0.6734, Accuracy = 65.73%\n",
      "Epoch 012: Loss = 0.6646, Accuracy = 68.96%\n",
      "Epoch 013: Loss = 0.6513, Accuracy = 71.23%\n",
      "Epoch 014: Loss = 0.6314, Accuracy = 73.04%\n",
      "Epoch 015: Loss = 0.6026, Accuracy = 74.58%\n",
      "Epoch 016: Loss = 0.5636, Accuracy = 76.22%\n",
      "Epoch 017: Loss = 0.5182, Accuracy = 77.62%\n",
      "Epoch 018: Loss = 0.4804, Accuracy = 78.31%\n",
      "Epoch 019: Loss = 0.4604, Accuracy = 78.79%\n",
      "Epoch 020: Loss = 0.4469, Accuracy = 79.84%\n",
      "Epoch 021: Loss = 0.4237, Accuracy = 81.04%\n",
      "Epoch 022: Loss = 0.3999, Accuracy = 82.60%\n",
      "Epoch 023: Loss = 0.3968, Accuracy = 82.40%\n",
      "Epoch 024: Loss = 0.4745, Accuracy = 79.31%\n",
      "Epoch 025: Loss = 1.0409, Accuracy = 57.45%\n",
      "Epoch 026: Loss = 1.0140, Accuracy = 49.56%\n",
      "Epoch 027: Loss = 0.6399, Accuracy = 51.44%\n",
      "Epoch 028: Loss = 0.6865, Accuracy = 61.42%\n",
      "Epoch 029: Loss = 0.6887, Accuracy = 50.99%\n",
      "Epoch 030: Loss = 0.6839, Accuracy = 50.99%\n",
      "Epoch 031: Loss = 0.6745, Accuracy = 50.99%\n",
      "Epoch 032: Loss = 0.6561, Accuracy = 58.04%\n",
      "Epoch 033: Loss = 0.6187, Accuracy = 69.87%\n",
      "Epoch 034: Loss = 0.5643, Accuracy = 73.50%\n",
      "Epoch 035: Loss = 0.5182, Accuracy = 76.34%\n",
      "Epoch 036: Loss = 0.4538, Accuracy = 79.31%\n",
      "Epoch 037: Loss = 0.4493, Accuracy = 79.90%\n",
      "Epoch 038: Loss = 0.4482, Accuracy = 80.45%\n",
      "Epoch 039: Loss = 0.4538, Accuracy = 79.95%\n",
      "Epoch 040: Loss = 0.4047, Accuracy = 82.02%\n",
      "Epoch 041: Loss = 0.3992, Accuracy = 82.30%\n",
      "Epoch 042: Loss = 0.3812, Accuracy = 83.42%\n",
      "Epoch 043: Loss = 0.3878, Accuracy = 83.22%\n",
      "Epoch 044: Loss = 0.3758, Accuracy = 83.94%\n",
      "Epoch 045: Loss = 0.3706, Accuracy = 83.92%\n",
      "Epoch 046: Loss = 0.3571, Accuracy = 84.49%\n",
      "Epoch 047: Loss = 0.3547, Accuracy = 84.61%\n",
      "Epoch 048: Loss = 0.3472, Accuracy = 85.03%\n",
      "Epoch 049: Loss = 0.3395, Accuracy = 85.35%\n",
      "Epoch 050: Loss = 0.3319, Accuracy = 85.61%\n",
      "Epoch 051: Loss = 0.3213, Accuracy = 86.23%\n",
      "Epoch 052: Loss = 0.3140, Accuracy = 86.74%\n",
      "Epoch 053: Loss = 0.3063, Accuracy = 87.28%\n",
      "Epoch 054: Loss = 0.2962, Accuracy = 87.93%\n",
      "Epoch 055: Loss = 0.2865, Accuracy = 88.32%\n",
      "Epoch 056: Loss = 0.2763, Accuracy = 88.80%\n",
      "Epoch 057: Loss = 0.2655, Accuracy = 89.29%\n",
      "Epoch 058: Loss = 0.2544, Accuracy = 89.88%\n",
      "Epoch 059: Loss = 0.2434, Accuracy = 90.59%\n",
      "Epoch 060: Loss = 0.2327, Accuracy = 91.15%\n",
      "Epoch 061: Loss = 0.2227, Accuracy = 91.67%\n",
      "Epoch 062: Loss = 0.2149, Accuracy = 92.16%\n",
      "Epoch 063: Loss = 0.2199, Accuracy = 91.54%\n",
      "Epoch 064: Loss = 0.3142, Accuracy = 87.66%\n",
      "Epoch 065: Loss = 1.1849, Accuracy = 63.49%\n",
      "Epoch 066: Loss = 1.8214, Accuracy = 49.01%\n",
      "Epoch 067: Loss = 0.6383, Accuracy = 70.06%\n",
      "Epoch 068: Loss = 0.6926, Accuracy = 51.13%\n",
      "Epoch 069: Loss = 0.6926, Accuracy = 50.99%\n",
      "Epoch 070: Loss = 0.6917, Accuracy = 50.99%\n",
      "Epoch 071: Loss = 0.6899, Accuracy = 50.99%\n",
      "Epoch 072: Loss = 0.6858, Accuracy = 50.99%\n",
      "Epoch 073: Loss = 0.6765, Accuracy = 66.52%\n",
      "Epoch 074: Loss = 0.6577, Accuracy = 80.38%\n",
      "Epoch 075: Loss = 0.6302, Accuracy = 62.95%\n",
      "Epoch 076: Loss = 0.6072, Accuracy = 55.82%\n",
      "Epoch 077: Loss = 0.5637, Accuracy = 75.03%\n",
      "Epoch 078: Loss = 0.5099, Accuracy = 81.23%\n",
      "Epoch 079: Loss = 0.4653, Accuracy = 81.09%\n",
      "Epoch 080: Loss = 0.4454, Accuracy = 81.77%\n",
      "Epoch 081: Loss = 0.4285, Accuracy = 80.70%\n",
      "Epoch 082: Loss = 0.4595, Accuracy = 81.95%\n",
      "Epoch 083: Loss = 0.7316, Accuracy = 68.30%\n",
      "Epoch 084: Loss = 1.9635, Accuracy = 52.09%\n",
      "Epoch 085: Loss = 0.5027, Accuracy = 82.77%\n",
      "Epoch 086: Loss = 0.6234, Accuracy = 62.21%\n",
      "Epoch 087: Loss = 0.6454, Accuracy = 51.46%\n",
      "Epoch 088: Loss = 0.6487, Accuracy = 50.99%\n",
      "Epoch 089: Loss = 0.6462, Accuracy = 50.99%\n",
      "Epoch 090: Loss = 0.6337, Accuracy = 50.99%\n",
      "Epoch 091: Loss = 0.6053, Accuracy = 50.99%\n",
      "Epoch 092: Loss = 0.5633, Accuracy = 50.99%\n",
      "Epoch 093: Loss = 0.5104, Accuracy = 75.31%\n",
      "Epoch 094: Loss = 0.4493, Accuracy = 80.24%\n",
      "Epoch 095: Loss = 0.4052, Accuracy = 80.98%\n",
      "Epoch 096: Loss = 0.3938, Accuracy = 80.98%\n",
      "Epoch 097: Loss = 0.3650, Accuracy = 82.31%\n",
      "Epoch 098: Loss = 0.3522, Accuracy = 83.84%\n",
      "Epoch 099: Loss = 0.3206, Accuracy = 86.59%\n",
      "Epoch 100: Loss = 0.3053, Accuracy = 87.71%\n",
      "Epoch 101: Loss = 0.3000, Accuracy = 88.78%\n",
      "Epoch 102: Loss = 0.2867, Accuracy = 89.89%\n",
      "Epoch 103: Loss = 0.2753, Accuracy = 90.22%\n",
      "Epoch 104: Loss = 0.2594, Accuracy = 90.78%\n",
      "Epoch 105: Loss = 0.2504, Accuracy = 91.02%\n",
      "Epoch 106: Loss = 0.2370, Accuracy = 91.40%\n",
      "Epoch 107: Loss = 0.2257, Accuracy = 91.81%\n",
      "Epoch 108: Loss = 0.2168, Accuracy = 92.19%\n",
      "Epoch 109: Loss = 0.2069, Accuracy = 92.55%\n",
      "Epoch 110: Loss = 0.1986, Accuracy = 92.83%\n",
      "Epoch 111: Loss = 0.1909, Accuracy = 93.16%\n",
      "Epoch 112: Loss = 0.1836, Accuracy = 93.38%\n",
      "Epoch 113: Loss = 0.1773, Accuracy = 93.61%\n",
      "Epoch 114: Loss = 0.1717, Accuracy = 93.88%\n",
      "Epoch 115: Loss = 0.1662, Accuracy = 94.01%\n",
      "Epoch 116: Loss = 0.1614, Accuracy = 94.17%\n",
      "Epoch 117: Loss = 0.1572, Accuracy = 94.36%\n",
      "Epoch 118: Loss = 0.1535, Accuracy = 94.52%\n",
      "Epoch 119: Loss = 0.1500, Accuracy = 94.65%\n",
      "Epoch 120: Loss = 0.1472, Accuracy = 94.78%\n",
      "Epoch 121: Loss = 0.1444, Accuracy = 94.82%\n",
      "Epoch 122: Loss = 0.1417, Accuracy = 94.96%\n",
      "Epoch 123: Loss = 0.1394, Accuracy = 95.10%\n",
      "Epoch 124: Loss = 0.1373, Accuracy = 95.18%\n",
      "Epoch 125: Loss = 0.1352, Accuracy = 95.27%\n",
      "Epoch 126: Loss = 0.1334, Accuracy = 95.33%\n",
      "Epoch 127: Loss = 0.1316, Accuracy = 95.40%\n",
      "Epoch 128: Loss = 0.1299, Accuracy = 95.45%\n",
      "Epoch 129: Loss = 0.1284, Accuracy = 95.50%\n",
      "Epoch 130: Loss = 0.1269, Accuracy = 95.57%\n",
      "Epoch 131: Loss = 0.1255, Accuracy = 95.63%\n",
      "Epoch 132: Loss = 0.1242, Accuracy = 95.69%\n",
      "Epoch 133: Loss = 0.1229, Accuracy = 95.74%\n",
      "Epoch 134: Loss = 0.1217, Accuracy = 95.78%\n",
      "Epoch 135: Loss = 0.1206, Accuracy = 95.82%\n",
      "Epoch 136: Loss = 0.1195, Accuracy = 95.86%\n",
      "Epoch 137: Loss = 0.1184, Accuracy = 95.92%\n",
      "Epoch 138: Loss = 0.1173, Accuracy = 95.96%\n",
      "Epoch 139: Loss = 0.1163, Accuracy = 96.02%\n",
      "Epoch 140: Loss = 0.1153, Accuracy = 96.04%\n",
      "Epoch 141: Loss = 0.1144, Accuracy = 96.08%\n",
      "Epoch 142: Loss = 0.1135, Accuracy = 96.11%\n",
      "Epoch 143: Loss = 0.1126, Accuracy = 96.11%\n",
      "Epoch 144: Loss = 0.1117, Accuracy = 96.14%\n",
      "Epoch 145: Loss = 0.1109, Accuracy = 96.17%\n",
      "Epoch 146: Loss = 0.1101, Accuracy = 96.18%\n",
      "Epoch 147: Loss = 0.1093, Accuracy = 96.21%\n",
      "Epoch 148: Loss = 0.1086, Accuracy = 96.26%\n",
      "Epoch 149: Loss = 0.1078, Accuracy = 96.29%\n",
      "Epoch 150: Loss = 0.1072, Accuracy = 96.31%\n",
      "Epoch 151: Loss = 0.1065, Accuracy = 96.35%\n",
      "Epoch 152: Loss = 0.1061, Accuracy = 96.35%\n",
      "Epoch 153: Loss = 0.1060, Accuracy = 96.35%\n",
      "Epoch 154: Loss = 0.1068, Accuracy = 96.34%\n",
      "Epoch 155: Loss = 0.1100, Accuracy = 96.09%\n",
      "Epoch 156: Loss = 0.1186, Accuracy = 95.85%\n",
      "Epoch 157: Loss = 0.1467, Accuracy = 94.49%\n",
      "Epoch 158: Loss = 0.2041, Accuracy = 92.33%\n",
      "Epoch 159: Loss = 0.3423, Accuracy = 86.43%\n",
      "Epoch 160: Loss = 0.1892, Accuracy = 92.17%\n",
      "Epoch 161: Loss = 0.2678, Accuracy = 87.39%\n",
      "Epoch 162: Loss = 0.2157, Accuracy = 92.59%\n",
      "Epoch 163: Loss = 0.2130, Accuracy = 92.55%\n",
      "Epoch 164: Loss = 0.1848, Accuracy = 93.39%\n",
      "Epoch 165: Loss = 0.1392, Accuracy = 94.81%\n",
      "Epoch 166: Loss = 0.1587, Accuracy = 94.10%\n",
      "Epoch 167: Loss = 0.1302, Accuracy = 95.30%\n",
      "Epoch 168: Loss = 0.1409, Accuracy = 95.05%\n",
      "Epoch 169: Loss = 0.1296, Accuracy = 95.47%\n",
      "Epoch 170: Loss = 0.1274, Accuracy = 95.53%\n",
      "Epoch 171: Loss = 0.1158, Accuracy = 95.88%\n",
      "Epoch 172: Loss = 0.1172, Accuracy = 95.76%\n",
      "Epoch 173: Loss = 0.1101, Accuracy = 96.10%\n",
      "Epoch 174: Loss = 0.1107, Accuracy = 96.03%\n",
      "Epoch 175: Loss = 0.1089, Accuracy = 96.12%\n",
      "Epoch 176: Loss = 0.1061, Accuracy = 96.24%\n",
      "Epoch 177: Loss = 0.1034, Accuracy = 96.36%\n",
      "Epoch 178: Loss = 0.1030, Accuracy = 96.39%\n",
      "Epoch 179: Loss = 0.1016, Accuracy = 96.44%\n",
      "Epoch 180: Loss = 0.1001, Accuracy = 96.53%\n",
      "Epoch 181: Loss = 0.0994, Accuracy = 96.53%\n",
      "Epoch 182: Loss = 0.0984, Accuracy = 96.62%\n",
      "Epoch 183: Loss = 0.0970, Accuracy = 96.64%\n",
      "Epoch 184: Loss = 0.0960, Accuracy = 96.66%\n",
      "Epoch 185: Loss = 0.0954, Accuracy = 96.68%\n",
      "Epoch 186: Loss = 0.0946, Accuracy = 96.69%\n",
      "Epoch 187: Loss = 0.0940, Accuracy = 96.72%\n",
      "Epoch 188: Loss = 0.0933, Accuracy = 96.74%\n",
      "Epoch 189: Loss = 0.0925, Accuracy = 96.77%\n",
      "Epoch 190: Loss = 0.0918, Accuracy = 96.84%\n",
      "Epoch 191: Loss = 0.0912, Accuracy = 96.85%\n",
      "Epoch 192: Loss = 0.0907, Accuracy = 96.88%\n",
      "Epoch 193: Loss = 0.0900, Accuracy = 96.89%\n",
      "Epoch 194: Loss = 0.0895, Accuracy = 96.92%\n",
      "Epoch 195: Loss = 0.0889, Accuracy = 96.95%\n",
      "Epoch 196: Loss = 0.0884, Accuracy = 96.96%\n",
      "Epoch 197: Loss = 0.0879, Accuracy = 96.98%\n",
      "Epoch 198: Loss = 0.0873, Accuracy = 97.00%\n",
      "Epoch 199: Loss = 0.0868, Accuracy = 97.01%\n",
      "Epoch 200: Loss = 0.0863, Accuracy = 97.03%\n"
     ]
    }
   ],
   "source": [
    "from EVO import GradientDescentOptimizer\n",
    "\n",
    "# Define model\n",
    "input_dim = X_train_exp.shape[1]\n",
    "model = DeepNeuralNetwork(layer_dims=[input_dim, 128, 64, 1])\n",
    "\n",
    "# Attach gradient descent optimizer\n",
    "optimizer = GradientDescentOptimizer(model)\n",
    "model.set_optimizer(optimizer)\n",
    "\n",
    "# Move data to device\n",
    "X_train_exp = X_train_exp.to(device)\n",
    "y_train = y_train.to(device)\n",
    "\n",
    "# Initialize weights if not done yet\n",
    "if model.w is None:\n",
    "    param_size = sum(in_dim * out_dim + out_dim for in_dim, out_dim in zip(model.layer_dims[:-1], model.layer_dims[1:]))\n",
    "    model.w = torch.rand(param_size, device=device)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 200\n",
    "alpha = 0.05  # learning rate\n",
    "beta = 0.8   # momentum\n",
    "\n",
    "losses = []\n",
    "train_accs = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.step(X_train_exp, y_train, alpha, beta)\n",
    "\n",
    "    loss_val = model.loss(X_train_exp, y_train).item()\n",
    "    losses.append(loss_val)\n",
    "\n",
    "    preds = model.predict(X_train_exp)\n",
    "    acc = (preds == y_train).float().mean().item() * 100\n",
    "    train_accs.append(acc)\n",
    "\n",
    "    print(f\"Epoch {epoch+1:03d}: Loss = {loss_val:.4f}, Accuracy = {acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622b725f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy (Most Diverse 4 - Majority Vote): 77.87%\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import torch\n",
    "\n",
    "def most_diverse_subset_vote_accuracy(model, population, X, y, subset_size=4):\n",
    "    \"\"\"\n",
    "    Compute test accuracy using majority voting among the most diverse subset of weight vectors.\n",
    "    Diversity is measured by average pairwise distance.\n",
    "\n",
    "    Args:\n",
    "        model: model with forward(X, w)\n",
    "        population: list of weight tensors\n",
    "        X: test data (N, D)\n",
    "        y: test labels (N,)\n",
    "        subset_size: number of individuals to use for voting\n",
    "\n",
    "    Returns:\n",
    "        accuracy (float, in percent)\n",
    "    \"\"\"\n",
    "    best_diversity = -float('inf')\n",
    "    best_subset = None\n",
    "\n",
    "    for subset in itertools.combinations(population, subset_size):\n",
    "        # Compute average pairwise distance for this subset\n",
    "        dists = []\n",
    "        for i in range(subset_size):\n",
    "            for j in range(i+1, subset_size):\n",
    "                dists.append(torch.norm(subset[i] - subset[j]))\n",
    "        avg_dist = torch.stack(dists).mean().item()\n",
    "\n",
    "        if avg_dist > best_diversity:\n",
    "            best_diversity = avg_dist\n",
    "            best_subset = subset\n",
    "\n",
    "    # Get predictions from the most diverse subset\n",
    "    votes = []\n",
    "    for w in best_subset:\n",
    "        preds = (model.forward(X, w) > 0.5).float()\n",
    "        votes.append(preds)\n",
    "\n",
    "    vote_tensor = torch.stack(votes)\n",
    "    majority_preds = (vote_tensor.sum(dim=0) > (subset_size / 2)).float()\n",
    "    acc = (majority_preds == y).float().mean().item() * 100\n",
    "\n",
    "    return acc\n",
    "\n",
    "# Ensure X_test_exp and y_test are on the same device as the model\n",
    "X_test_exp = X_test_exp.to(device)\n",
    "y_test = y_test.to(device)\n",
    "\n",
    "acc_diverse4 = most_diverse_subset_vote_accuracy(model, optimizer.population, X_test_exp, y_test, subset_size=3)\n",
    "print(f\"Test Accuracy (Most Diverse  - Majority Vote): {acc_diverse4:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0285ba28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df2dda8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Torch SGD] Epoch 0001: Loss = 0.9153, Training Accuracy = 42.54%\n",
      "[Torch SGD] Epoch 0100: Loss = 0.6773, Training Accuracy = 58.06%\n",
      "[Torch SGD] Epoch 0200: Loss = 0.5643, Training Accuracy = 72.28%\n",
      "[Torch SGD] Epoch 0300: Loss = 0.5046, Training Accuracy = 77.49%\n",
      "[Torch SGD] Epoch 0400: Loss = 0.4678, Training Accuracy = 80.09%\n",
      "[Torch SGD] Epoch 0500: Loss = 0.4430, Training Accuracy = 81.46%\n",
      "[Torch SGD] Epoch 0600: Loss = 0.4254, Training Accuracy = 82.39%\n",
      "[Torch SGD] Epoch 0700: Loss = 0.4124, Training Accuracy = 82.99%\n",
      "[Torch SGD] Epoch 0800: Loss = 0.4025, Training Accuracy = 83.41%\n",
      "[Torch SGD] Epoch 0900: Loss = 0.3947, Training Accuracy = 83.73%\n",
      "[Torch SGD] Epoch 1000: Loss = 0.3883, Training Accuracy = 83.97%\n",
      "[Torch SGD] Epoch 1100: Loss = 0.3831, Training Accuracy = 84.17%\n",
      "[Torch SGD] Epoch 1200: Loss = 0.3787, Training Accuracy = 84.31%\n",
      "[Torch SGD] Epoch 1300: Loss = 0.3749, Training Accuracy = 84.49%\n",
      "[Torch SGD] Epoch 1400: Loss = 0.3716, Training Accuracy = 84.68%\n",
      "[Torch SGD] Epoch 1500: Loss = 0.3687, Training Accuracy = 84.79%\n",
      "[Torch SGD] Epoch 1600: Loss = 0.3661, Training Accuracy = 84.92%\n",
      "[Torch SGD] Epoch 1700: Loss = 0.3637, Training Accuracy = 85.06%\n",
      "[Torch SGD] Epoch 1800: Loss = 0.3615, Training Accuracy = 85.18%\n",
      "[Torch SGD] Epoch 1900: Loss = 0.3595, Training Accuracy = 85.30%\n",
      "[Torch SGD] Epoch 2000: Loss = 0.3577, Training Accuracy = 85.37%\n",
      "[Torch SGD] Epoch 2100: Loss = 0.3559, Training Accuracy = 85.44%\n",
      "[Torch SGD] Epoch 2200: Loss = 0.3543, Training Accuracy = 85.54%\n",
      "[Torch SGD] Epoch 2300: Loss = 0.3528, Training Accuracy = 85.58%\n",
      "[Torch SGD] Epoch 2400: Loss = 0.3514, Training Accuracy = 85.67%\n",
      "[Torch SGD] Epoch 2500: Loss = 0.3500, Training Accuracy = 85.77%\n",
      "[Torch SGD] Epoch 2600: Loss = 0.3487, Training Accuracy = 85.84%\n",
      "[Torch SGD] Epoch 2700: Loss = 0.3475, Training Accuracy = 85.92%\n",
      "[Torch SGD] Epoch 2800: Loss = 0.3464, Training Accuracy = 85.98%\n",
      "[Torch SGD] Epoch 2900: Loss = 0.3453, Training Accuracy = 86.01%\n",
      "[Torch SGD] Epoch 3000: Loss = 0.3442, Training Accuracy = 86.11%\n",
      "[Torch SGD] Epoch 3100: Loss = 0.3432, Training Accuracy = 86.16%\n",
      "[Torch SGD] Epoch 3200: Loss = 0.3423, Training Accuracy = 86.22%\n",
      "[Torch SGD] Epoch 3300: Loss = 0.3414, Training Accuracy = 86.27%\n",
      "[Torch SGD] Epoch 3400: Loss = 0.3405, Training Accuracy = 86.32%\n",
      "[Torch SGD] Epoch 3500: Loss = 0.3397, Training Accuracy = 86.37%\n",
      "[Torch SGD] Epoch 3600: Loss = 0.3389, Training Accuracy = 86.43%\n",
      "[Torch SGD] Epoch 3700: Loss = 0.3381, Training Accuracy = 86.50%\n",
      "[Torch SGD] Epoch 3800: Loss = 0.3374, Training Accuracy = 86.55%\n",
      "[Torch SGD] Epoch 3900: Loss = 0.3367, Training Accuracy = 86.61%\n",
      "[Torch SGD] Epoch 4000: Loss = 0.3361, Training Accuracy = 86.66%\n",
      "[Torch SGD] Epoch 4100: Loss = 0.3354, Training Accuracy = 86.70%\n",
      "[Torch SGD] Epoch 4200: Loss = 0.3348, Training Accuracy = 86.72%\n",
      "[Torch SGD] Epoch 4300: Loss = 0.3342, Training Accuracy = 86.77%\n",
      "[Torch SGD] Epoch 4400: Loss = 0.3337, Training Accuracy = 86.78%\n",
      "[Torch SGD] Epoch 4500: Loss = 0.3332, Training Accuracy = 86.81%\n",
      "[Torch SGD] Epoch 4600: Loss = 0.3327, Training Accuracy = 86.84%\n",
      "[Torch SGD] Epoch 4700: Loss = 0.3322, Training Accuracy = 86.86%\n",
      "[Torch SGD] Epoch 4800: Loss = 0.3317, Training Accuracy = 86.88%\n",
      "[Torch SGD] Epoch 4900: Loss = 0.3313, Training Accuracy = 86.89%\n",
      "[Torch SGD] Epoch 5000: Loss = 0.3308, Training Accuracy = 86.88%\n",
      "[Torch SGD] Epoch 5100: Loss = 0.3304, Training Accuracy = 86.91%\n",
      "[Torch SGD] Epoch 5200: Loss = 0.3300, Training Accuracy = 86.91%\n",
      "[Torch SGD] Epoch 5300: Loss = 0.3297, Training Accuracy = 86.92%\n",
      "[Torch SGD] Epoch 5400: Loss = 0.3293, Training Accuracy = 86.93%\n",
      "[Torch SGD] Epoch 5500: Loss = 0.3290, Training Accuracy = 86.95%\n",
      "[Torch SGD] Epoch 5600: Loss = 0.3287, Training Accuracy = 87.00%\n",
      "[Torch SGD] Epoch 5700: Loss = 0.3284, Training Accuracy = 87.04%\n",
      "[Torch SGD] Epoch 5800: Loss = 0.3281, Training Accuracy = 87.05%\n",
      "[Torch SGD] Epoch 5900: Loss = 0.3278, Training Accuracy = 87.07%\n",
      "[Torch SGD] Epoch 6000: Loss = 0.3275, Training Accuracy = 87.08%\n",
      "[Torch SGD] Epoch 6100: Loss = 0.3273, Training Accuracy = 87.12%\n",
      "[Torch SGD] Epoch 6200: Loss = 0.3270, Training Accuracy = 87.14%\n",
      "[Torch SGD] Epoch 6300: Loss = 0.3268, Training Accuracy = 87.14%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 39\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m epoch \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     38\u001b[0m         preds \u001b[38;5;241m=\u001b[39m (outputs \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m---> 39\u001b[0m         acc \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m     40\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Torch SGD] Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m04d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Training Accuracy = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     42\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "# Define a simple logistic regression model using PyTorch's built-in modules\n",
    "class TorchLogisticRegression(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, 1)  # Binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.linear(x))\n",
    "\n",
    "# Move model and data to device\n",
    "input_dim = X_train_exp.shape[1]\n",
    "torch_model = TorchLogisticRegression(input_dim).to(device)\n",
    "X = X_train_exp.to(device)\n",
    "y = y_train.float().unsqueeze(1).to(device)  # Reshape to (N, 1)\n",
    "\n",
    "# Use SGD optimizer\n",
    "optimizer = optim.\n",
    "\n",
    "# Binary cross entropy loss\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 20000\n",
    "start_time = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = torch_model(X)\n",
    "    loss = criterion(outputs, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 100 == 0 or epoch == 0:\n",
    "        preds = (outputs > 0.5).float()\n",
    "        acc = (preds == y).float().mean().item() * 100\n",
    "        print(f\"[Torch SGD] Epoch {epoch+1:04d}: Loss = {loss.item():.4f}, Training Accuracy = {acc:.2f}%\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"\\nTotal Training Time: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7774ffb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the losses over epochs\n",
    "plt.plot((diversity_loss[1:]), label=\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss over Epochs\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39885ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_list = []\n",
    "for candidate in model.optimizer.population:\n",
    "    model.w = candidate  # Set the model's weight to the candidate.\n",
    "    acc = (model.predict(X_test_exp).squeeze() == y_test.to(device)).float().mean().item()\n",
    "    accuracy_list.append((candidate, acc))\n",
    "\n",
    "final_pop = [candidate for candidate, _ in sorted(accuracy_list, key=lambda pair: pair[1], reverse=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed8cad5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(153.5215, device='mps:0')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def average_pairwise_distance(population):\n",
    "    n = len(population)\n",
    "    total_dist = 0\n",
    "    count = 0\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            dist = torch.norm(population[i] - population[j])\n",
    "            total_dist += dist\n",
    "            count += 1\n",
    "\n",
    "    return total_dist / count if count > 0 else 0\n",
    "\n",
    "average_pairwise_distance(optimizer.population)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f3897b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def population_majority_vote_accuracy(model, X, y):\n",
    "    \"\"\"\n",
    "    For each sample, collect predictions from all individuals in the population,\n",
    "    take the majority vote, and compare to ground truth labels.\n",
    "    \"\"\"\n",
    "    n = X.size(0)\n",
    "    votes = torch.zeros((len(model.population), n))\n",
    "\n",
    "    for i, w in enumerate(model.population):\n",
    "        logits = X @ w\n",
    "        preds = (logits > 0).float()\n",
    "        votes[i] = preds\n",
    "\n",
    "    # Sum over voters and apply majority rule (>50%)\n",
    "    majority_preds = (votes.mean(dim=0) > 0.5).float()\n",
    "    accuracy = (majority_preds == y).float().mean().item() * 100\n",
    "    return accuracy\n",
    "\n",
    "def average_pairwise_distance(population):\n",
    "    n = len(population)\n",
    "    total_dist = 0\n",
    "    count = 0\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            dist = torch.norm(population[i] - population[j])\n",
    "            total_dist += dist\n",
    "            count += 1\n",
    "\n",
    "    return total_dist / count if count > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a1c2b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'optimizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 75\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pair\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Get the two most diverse individuals from your optimizer's population.\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m indiv_A, indiv_B \u001b[38;5;241m=\u001b[39m find_most_diverse_pair(\u001b[43moptimizer\u001b[49m\u001b[38;5;241m.\u001b[39mpopulation)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# For visualization, remove the bias column from X_test.\u001b[39;00m\n\u001b[1;32m     78\u001b[0m original_val_images \u001b[38;5;241m=\u001b[39m X_test[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# shape: (N_test, 784)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'optimizer' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch\n",
    "\n",
    "def visualize_predictions_on_random_samples(X_val_exp, y_val, original_images, individual_A, individual_B, model, n=10):\n",
    "    \"\"\"\n",
    "    Visualize n random samples from the validation/test set using predictions\n",
    "    from two candidate individuals.\n",
    "    \n",
    "    Args:\n",
    "      X_val_exp (Tensor): Expanded validation features (shape: [N, 7850]).\n",
    "      y_val (Tensor): True labels (shape: [N]).\n",
    "      original_images (Tensor): Original validation images with bias removed \n",
    "                                (shape: [N, 785]), where first 784 values are the image.\n",
    "      individual_A (Tensor): Candidate weight vector A (shape: [7850]).\n",
    "      individual_B (Tensor): Candidate weight vector B (shape: [7850]).\n",
    "      model: Instance of MultiClassLogisticRegressionWrapper.\n",
    "      n (int): Number of random samples to display.\n",
    "    \"\"\"\n",
    "    # Use X_val_exp's size (not X_val) to sample indices.\n",
    "    indices = random.sample(range(X_val_exp.size(0)), n)\n",
    "\n",
    "    # Create a grid of subplots.\n",
    "    fig, axs = plt.subplots(n, 3, figsize=(8, n * 2))\n",
    "    fig.suptitle(\"Predictions: True Label vs. Individuals' Predictions\", fontsize=16)\n",
    "\n",
    "    for i, idx in enumerate(indices):\n",
    "        # Get the expanded input for candidate prediction (shape: (1, 7850)).\n",
    "        x = X_val_exp[idx].unsqueeze(0)\n",
    "        \n",
    "        # For visualization: use the original image (remove bias column) and reshape to 28x28.\n",
    "        img = original_images[idx].view(28, 28)\n",
    "        true_label = int(y_val[idx].item())\n",
    "\n",
    "        # Predict with Individual A using the candidate weight vector.\n",
    "        scores_A = model.score_with(x, individual_A)  # shape: (1, 10)\n",
    "        pred_A = torch.argmax(scores_A, dim=1).item()\n",
    "\n",
    "        # Predict with Individual B using the candidate weight vector.\n",
    "        scores_B = model.score_with(x, individual_B)\n",
    "        pred_B = torch.argmax(scores_B, dim=1).item()\n",
    "\n",
    "        # Plot the original image with true label.\n",
    "        axs[i, 0].imshow(img, cmap='gray')\n",
    "        axs[i, 0].axis('off')\n",
    "        axs[i, 0].set_title(f\"True: {true_label}\")\n",
    "\n",
    "        # Display Individual A's prediction.\n",
    "        axs[i, 1].text(0.5, 0.5, f\"{pred_A}\", fontsize=16, ha='center', va='center')\n",
    "        axs[i, 1].axis('off')\n",
    "        axs[i, 1].set_title(\"Individual A\")\n",
    "\n",
    "        # Display Individual B's prediction.\n",
    "        axs[i, 2].text(0.5, 0.5, f\"{pred_B}\", fontsize=16, ha='center', va='center')\n",
    "        axs[i, 2].axis('off')\n",
    "        axs[i, 2].set_title(\"Individual B\")\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "    plt.show()\n",
    "\n",
    "# --- Example usage ---\n",
    "# Assume you have defined `find_most_diverse_pair` function:\n",
    "def find_most_diverse_pair(population):\n",
    "    max_dist = 0\n",
    "    pair = (None, None)\n",
    "    for i in range(len(population)):\n",
    "        for j in range(i + 1, len(population)):\n",
    "            dist = torch.norm(population[i] - population[j])\n",
    "            if dist > max_dist:\n",
    "                max_dist = dist\n",
    "                pair = (population[i], population[j])\n",
    "    return pair\n",
    "\n",
    "# Get the two most diverse individuals from your optimizer's population.\n",
    "indiv_A, indiv_B = find_most_diverse_pair(optimizer.population)\n",
    "\n",
    "# For visualization, remove the bias column from X_test.\n",
    "original_val_images = X_test[:, :-1]  # shape: (N_test, 784)\n",
    "\n",
    "# Now call the visualization function with your test set.\n",
    "visualize_predictions_on_random_samples(X_test_exp, y_test, original_val_images, indiv_A, indiv_B, model, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b075615",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
