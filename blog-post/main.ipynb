{
 "cells": [
  {
   "cell_type": "raw",
   "id": "07689bb5",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: Evolution Based Weight Vector Optimization\n",
    "author: James Cummings, Jiffy Lesica, Yahya Rahhawi,Lukka Wolff\n",
    "date: '2025-19-05'\n",
    "image: \"image.jpg\"\n",
    "description: \"Implementation of Evolution Based Weight Vector Optimization\"\n",
    "bibliography: refs.bib\n",
    "format: html\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b96871",
   "metadata": {},
   "source": [
    "# Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c536edd",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This blog post explores how features of evolution in nature can inspire solutions to overcoming the shortcomings of gradient descent. Gradient Descent only works on differentiable loss functions, meaning it can become stuck in local loss minima when attempting to model non-convex loss functions. In other words, gradient descent cannot explore the entire solution space on nondifferentiable loss functions. This limitation can be overcome by harnessing the characteristics of evolution and natural selection in nature. Evolution has a wide variety of applications concerning Machine Learning, but this project focuses on its applications to weight vector optimization @Talikani2021evolutionary.\n",
    "\n",
    "\n",
    "Lewontin identifies 3 key population characteristics for evolution: phenotypic variation in a population, differential fitness, and fitness must be heritable @Lewontin1970units. With these 3 characteristics, evolution then occurs as ‘fitter’ individuals are better able to pass on their traits to future generations, while less fit individuals are not. At the individual level, evolution requires a blueprint, self-replication, mutation, and selection. By applying these principles to machine learning models, this blog post explores the strengths and limitations of evolutionary principles when applied to weight vector optimization in machine learning. To satisfy the requirement of phenotypic variation, each evolutionary optimizer has an entire population of weight vectors storing different weights. The different weights result in different losses, which in combination with selection pressures regarding the resulting different losses, satisfy the differential fitness requirement. With weight vectors serving as our genetic blueprint, those weight vectors can be duplicated to create or refill the population of weight vectors. Slight random adjustments to those weight vectors during replication serve as the mutations, ensuring the continuation of phenotypic variation. A variety of methods can be used to eliminate population vectors during an iteration, including loss and diversity, which function as selection. Eliminating high-loss weight vectors allows only vectors with high accuracy to pass on their characteristics, while eliminating low diversity can ensure that the solution space is adequately explored. Through the implementation of hyperparameters, many variations of evolutionary machine learning algorithms are explored to better understand their strengths and weaknesses.\n",
    "\n",
    "\n",
    "The many hyperparameters are then tested on both generated and real data from the MNIST dataset to develop initial hypotheses regarding the optimal parameterization for evolutionary weight vector optimization to succeed. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a71563c",
   "metadata": {},
   "source": [
    "# Values Statement\n",
    "\n",
    "The potential users of the evolutionary-based weight vector optimization class are researchers, data scientists, and developers, especially those who work on non-differentiable problems with which gradient descent-based solutions struggle. Our class provides both a potential solution to overcoming the limitations of gradient descent on non-differentiable classification problems and serves as a potential benchmark against which other algorithms can be compared. \n",
    "\n",
    "\n",
    "One major potential impact of the widespread use of our algorithm, or similar ones, is the increase in computational power required to run them. Because each epoch of an evolutionary algorithm requires the computation of an entire population of new weight vectors, the computational power required for an epoch is higher than most algorithms. This has potential positive implications for the manufacturers of computational chips and the owners of servers. On the other hand, the potential negative effects of increased energy and material consumption to perform these computations cannot be overlooked either. \n",
    "\n",
    "\n",
    "Because the majority of our work was focused on the creation of a class, and not the optimization of a specific algorithm, the potential for positive and negative impacts of our class depends on who gains access to the class and what they decide to do with it. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc39b496",
   "metadata": {},
   "source": [
    "# Materials and Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb76753",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5f66bc",
   "metadata": {},
   "source": [
    "### Proof of concept/vanilla evolution:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49edadd",
   "metadata": {},
   "source": [
    "### Selection/Elitism/Sneakers:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9d728a",
   "metadata": {},
   "source": [
    "### Inheritance and Parent Quantity:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c63324",
   "metadata": {},
   "source": [
    "At each iteration of our evolutionary optimization, following the creation of a ‘gene pool’ in the selection stage, the population must be replenished with new individuals. There are three ways that this can be accomplished. 1: All new individuals are new randomized weight vectors with no input from the gene pool. 2: Each new individual has a single parent randomly selected from the gene pool from which its weights are inherited with random mutations. 3: Each individual has n parents randomly selected from the gene pool. Each feature weight is then inherited from the corresponding feature weight of a random one of its parents. \n",
    "\n",
    "The first scenario, with no inherited weight vectors, is a baseline against which our true evolutionary models can be tested. This is not truly evolution, as it does not include any heritability of fitness for new individuals in the population @Lewontin1970units.\n",
    "\n",
    "The second Scenario, includes heritability of fitness, but with only a single parent for each child individual, the diversity can be expected to be more limited.\n",
    "\n",
    "<img src=\"images/Multi-Parent/Single_Parent_Inheritance.png\" width=\"400\" style=\"display:block; margin:auto;\" />\n",
    "\n",
    "<p style=\"text-align: center;\"><em>Diagram of Inheritance when num_parents = 1</em></p>\n",
    "\n",
    "\n",
    "\n",
    "The Third Scenario, allows for a slightly reduced heritability of fitness, with the addition of diverse new individuals produced with each generation. \n",
    "\n",
    "<img src=\"images/Multi-Parent/Multi-parent_Inheritance.png\" width=\"400\" style=\"display:block; margin:auto;\" />\n",
    "\n",
    "<p style=\"text-align: center;\"><em>Diagram of Inheritance when num_parents = 2</em></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef2acc1",
   "metadata": {},
   "source": [
    "As discussed in the results section, the choice of the number of parents can have sa significant impact on loss, accuracy, and diversity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2480a0a4",
   "metadata": {},
   "source": [
    "### Hybridizing evolution with gradient descent:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ffc1f4",
   "metadata": {},
   "source": [
    "### Adjustment from binary to multi-class classification:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb03f69",
   "metadata": {},
   "source": [
    "### Mutation Methods:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a59802",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decb09b3",
   "metadata": {},
   "source": [
    "### Proof of concept/vanilla evolution:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338b906f",
   "metadata": {},
   "source": [
    "### Selection/Elitism/Sneakers:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296a79b2",
   "metadata": {},
   "source": [
    "### Inheritance and Parent Quantity:\n",
    "\n",
    "##### Generated Data Experiment:\n",
    "\n",
    "As a proof of concept, a multi parent classification experiment was run on generated 2 dimensional data with 0.2 noise and 300 points. The accuracy, loss, and euclidean diversity was tracked across 300 iterations. The experiment was run with the hyperparameter num_parents set to 0, 1, 2, 3, 5, and 10.\n",
    "\n",
    "<img src=\"images/Multi-Parent/MP_GD_1_300.png\" width=\"900\" style=\"display:block; margin:auto;\" />\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"images/Multi-Parent/MP_GD_2_300.png\" width=\"450\" style=\"display:block; margin:auto;\" />\n",
    "<p style=\"text-align: center;\"><em>Figures demonstrating loss, diversity, and accuracy performance of Evolutionary Optimization on Generated data using 0, 1, 2, 3, 5, and 10 parents over 300 iterations</em></p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca59e86",
   "metadata": {},
   "source": [
    "As seen in the above visualization, Loss and Accuracy were comparable across all quantities of parents, while diversity varied significantly. In particular, with num_parents set to 0 and to a lesser extent 1, diversity was much lower than all other quantities of parents. The accuracy of the 0 parent model also lagged behind the other models over more iterations. Without functioning parents, evolution is replaced by random chance, as the heritability, defined as a requirement for evolution by @Lewontin1970units, is eliminated. \n",
    "\n",
    "While this had a much smaller impact on this relatively simple experiment of generated data, the implications on a much more complex classification problem, such as MNIST, could be significant. \n",
    "\n",
    "##### MNIST Experiment:\n",
    "\n",
    "A similar experiment, performed on a subset 1000 images from the MNIST dataset tested the accuracy, loss, and diversity of num_parents = 0, 1, 2, 5, 10 over 1000 iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94909bf",
   "metadata": {},
   "source": [
    "<img src=\"images/Multi-Parent/MP_MNIST_1.png\" width=\"900\" style=\"display:block; margin:auto;\" />\n",
    "\n",
    "\n",
    "<img src=\"images/Multi-Parent/MP_MNIST_2.png\" width=\"450\" style=\"display:block; margin:auto;\" />\n",
    "\n",
    "<p style=\"text-align: center;\"><em>Figures demonstrating loss, diversity, and accuracy performance of Evolutionary Optimization on a Subset of the MNIST dataset using 0, 1, 2, 5, and 10 parents over 1,000 iterations</em></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bd046c",
   "metadata": {},
   "source": [
    "The benefits of inheritance are clear, as the zero parent model has a significantly higher loss and lower accuracy throughout the 1000 iterations compared to all other models. Additionally, the benefits of multi-parent inheritance are demonstrated by the declining improvement in accuracy when num_parents = 1. The lower diversity compared to the other models, and the importance of diversity in evolutionary algorithms in allowing for the exploration of the solution space, results in poorer performance of the single parent model. A single parent allows for the inheritance of a fitness,leading to better performance compared to the 0 parent model  @Lewontin1970units. However, it does not allow for large enough variation in fitness. With lower diversity, the 1 parent model is less likely to find a global minimum compared to the 2+ parent models. While it does find some form of a local minimum, the lack of diversity results in a drop off in improvement at around 600 iterations, while the models with 2, 5, and 10 parents continue to have spikes in improvement. \n",
    "\n",
    "In the context of classification of the MNIST dataset, evolutionary models benefit from the added diversity resulting from the use of larger quantities of parents contributing weights to each new child in the subsequent generation. While more computing power, and more iterations are required to truly optimize this hyperparameter, these experiments clearly demonstrate the benefits of multi-parent inheritance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c550a420",
   "metadata": {},
   "source": [
    "### Hybridizing evolution with gradient descent:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e7fe4f",
   "metadata": {},
   "source": [
    "### Adjustment from binary to multi-class classification:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00dbbd2",
   "metadata": {},
   "source": [
    "### Mutation Methods:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2141ef80",
   "metadata": {},
   "source": [
    "### Final MNIST Results:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5151adf",
   "metadata": {},
   "source": [
    "# Concluding Discussion:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521d92e9",
   "metadata": {},
   "source": [
    "# Group Contributions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1a71cf",
   "metadata": {},
   "source": [
    "# Personal Reflection:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-0451",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
